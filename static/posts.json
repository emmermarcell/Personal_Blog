[{"id": 0, "date_posted": "2023-04-03", "title": "Remarkable representations of the 2 + 2 de sitter group", "tags": "Physics, Group Theory", "content": "A paper I have been working on for a while under my professor, Peter Levay. We have been investigating the different oscillator realizations of the AdS3 space. It is not a complete paper, more like a collection of thoughts, but it gives an idea about the working habits and the mindset of theoretical physicists when they work on a problem.", "paper": "../static/papers/Remarkable_representation_of_SU22.pdf"}, {"id": "1", "date_posted": "2023-04-08", "title": "Training and running a linear model using Scikit-Learn", "tags": "Machine Learning, Programming", "content": "<p>In this first demo example of my blog posts dealing with machine learning problems I introduce linear models, meaning that our model explicitly assumes a linear connection between the values of the feature vectors and the labels for it. This is the simplest example that is still considered a ML problem and it falls into the category of regression algorithms.</p>\r\n<p>I trained the model on the Better Life Index data to determine if money makes people happy. For example, say you want to know how happy Cypriots are, and the OECD data does not have the answer. Fortunately, you can use your model to make a good prediction: you look up Cyprus\u2019s GDP per capita, find $37,655, and then apply your model and find that life satisfaction is likely to be somewhere around 3.75 + 37,655 \u00d7 6.78 \u00d7 10\u207b\u2075 = 6.30.</p>\r\n<p>Then I used Matplotlib to visualize the Better Life Index data and the fit of the linear model.</p>\r\n\r\n<!-- The following code is written in Python and uses the libraries matplotlib.pyplot, pandas, and LinearRegression from sklearn.linear_model. -->\r\n\r\n<pre><code class=\"language-python\">\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nfrom sklearn.linear_model import LinearRegression\r\n\r\n\r\n# Download and prepare the data\r\ndata_root = \"https://github.com/ageron/data/raw/main/\"\r\nlifesat = pd.read_csv(data_root + \"lifesat/lifesat.csv\")\r\nX = lifesat[[\"GDP per capita (USD)\"]].values\r\ny = lifesat[[\"Life satisfaction\"]].values\r\n\r\n# Visualize the data\r\nlifesat.plot(kind='scatter', grid=True,\r\n             x=\"GDP per capita (USD)\", y=\"Life satisfaction\", label='Better Life Index data')\r\nplt.axis([23_500, 62_500, 4, 9])\r\nplt.title('Training and running a linear model using Scikit-Learn.py')\r\n\r\n# Select a linear model\r\nmodel = LinearRegression()\r\n\r\n# Train the model\r\nmodel.fit(X, y)\r\n\r\n# Make a prediction for Cyprus\r\nX_new = [[37_655.2]] # Cyprus' GDP per capita in 2020\r\nprint(model.predict(X_new)) # output: [[6.30165767]]\r\n\r\n# Plot the data and the model\r\nplt.plot(X, model.predict(X), color='red', label='model')\r\nplt.plot(X_new, model.predict(X_new), 'x', color='red', label='Prediction for Cyprus (6.30165767)')\r\nplt.legend()\r\n\r\n# Save the plot as a PNG file\r\nplt.savefig('Training and running a linear model using Scikit-Learn', dpi=300, bbox_inches='tight')\r\n</code></pre>\r\n\r\n<!-- The following is a plot generated by the code. -->\r\n\r\n<p>The output of this code is the following plot.</p>\r\n<img src='../static/pictures/Training and running a linear model using Scikit-Learn.png' style=\"display: block; margin: auto; max-width: 70%;\"><br>\r\n<br>\r\n<p>The code does the following things:</p>\r\n  <ol>\r\n  <li>Import the necessary libraries including <code>matplotlib.pyplot</code>, <code>pandas</code>, and <code>LinearRegression</code> from <code>sklearn.linear_model</code>.</li>\r\n  <li>Define the root URL where the data is stored and download the <code>lifesat.csv</code> file into a <code>pandas</code> DataFrame named <code>lifesat</code>.</li>\r\n  <li>Extract the <code>GDP per capita (USD)</code> and <code>Life satisfaction</code> columns from the <code>lifesat</code> DataFrame as <code>X</code> and <code>y</code> arrays respectively.</li>\r\n  <li>Create a scatter plot of the data using the <code>plot()</code> method of the <code>lifesat</code> DataFrame.</li>\r\n  <li>Limit the x-axis to the range <code>[23,500, 62,500]</code> and the y-axis to the range <code>[4, 9]</code>.</li>\r\n  <li>Set the title of the plot to \"Training and running a linear model using Scikit-Learn.py\".</li>\r\n  <li>Create a <code>LinearRegression</code> model object named <code>model</code>.</li>\r\n  <li>Train the <code>model</code> object on the <code>X</code> and <code>y</code> data using the <code>fit()</code> method.</li>\r\n  <li>Generate a prediction for Cyprus' GDP per capita in 2020 and store it in <code>X_new</code>.</li>\r\n  <li>Print the predicted life satisfaction score for Cyprus using the <code>predict()</code> method of the <code>model</code> object.</li>\r\n  <li>Add the regression line to the scatter plot by plotting the predicted values using <code>X</code> and <code>model.predict(X)</code>.</li>\r\n  <li>Plot the predicted value for Cyprus using <code>X_new</code> and <code>model.predict(X_new)</code>, and mark the point with a red <code>x</code> marker.</li>\r\n  <li>Add a legend to the plot.</li>\r\n  <li>Save the plot as a PNG file named \"Training and running a linear model using Scikit-Learn\" using the <code>savefig()</code> method.</li>\r\n</ol>\r\n\r\n<p>I referenced the book <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/\">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition</a> for this article.</p>", "paper": ""}, {"id": "2", "date_posted": "2023-04-11", "title": "Numeric Simulation for 1D and 2D quantum scattering in Python", "tags": "Programming, Physics", "content": "<p>I thought I will update this simulation I did with my friend and classmate, \u00c1ron M\u00e1rton a while ago in a Jupyter notebook. The problem that we tackled was the 1D and 2D scattering of a quantum wavepacket on different kinds of potentials.</p>\r\n<p>The comments for the code are in Hungarian but don't be afraid, I translated the introduction that describes the details of the numeric calculation needed for dealing with the discretized version of the Schr\u00f6dinger equation. Here it is</p>\r\n\r\n<h2>Time-dependent Schr\u00f6dinger equation in one dimension</h2>\r\n<p>$$ i \\hbar \\frac{\\partial}{\\partial t} \\psi(x,t) = \\hat{H} \\psi(x,t), \\quad \\text{where the Hamiltonian operator is:} \\quad \\hat{H} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2} + V(x) \\equiv \\hat{K} + \\hat{V} $$</p>\r\n\t<p>We can numerically solve the differential equation using the so-called <a href=\"https://www.algorithm-archive.org/contents/split-operator_method/split-operator_method.html\" target=\"_blank\">Split-operator</a> method, which determines the time evolution of the wave function in a small $\\Delta t$ time using the following equation:</p>\r\n\t<p>$$ \\psi(x, t + \\Delta t) \\approx \\exp \\left( -\\frac{i \\Delta t}{2\\hbar} \\hat{V}\\right) \r\n\t\\exp \\left( -\\frac{i\\Delta t}{\\hbar} \\hat{K} \\right) \r\n\t\\exp \\left( - \\frac{i \\Delta t}{2\\hbar} \\hat{V} \\right) \\psi(x,t) $$</p>\r\n\t<p>We first apply the kinetic energy operator in half a $\\Delta t/2$ time step, then the potential energy operator in a full $\\Delta t$ time step, and finally apply the kinetic energy operator again in half a time step.</p>\r\n\r\n<p>The advantage of the method is that we can treat the effects of $\\hat{K}$ and $\\hat{V}$ operators separately on the system. We can thus conveniently describe the effect of potential energy in coordinate representation, while the kinetic energy operator can be treated by performing a Fourier transform in momentum space:</p>\r\n\r\n\t<p>$$ \\hat{K} = -\\dfrac{\\hbar}{2m}\\frac{\\partial^2}{\\partial x^2} \\xrightarrow{\\mathcal{F}} \\hat{K_p} = \\frac{ p^2}{2m}  $$</p>\r\n\t<p>Thus, the performed operation is:</p>\r\n\t<p>$$ \\psi(x, t + \\Delta t) \\approx \\exp \\left( -\\frac{i \\Delta t}{2\\hbar} \\hat{V}\\right) \\mathcal{F^{-1}} \\left[\r\n\t\\exp \\left( -\\frac{i\\Delta t}{\\hbar} \\hat{K} \\right) \\mathcal{F} \\left[\r\n\t\\exp \\left( - \\frac{i \\Delta t}{2\\hbar} \\hat{V} \\right) \\psi(x,t) \\right] \\right] $$</p>\r\n\t<ul>\r\n\t\t<li>Numerical calculation error: $\\mathcal{O}(\\Delta t^3)$</li>\r\n\t\t<li>The division of the coordinate space: $$ x_n = x_\\text{min} + n \\cdot \\Delta x, \\qquad n \\in \\{0, 1, ... ,N-1 \\} $$ </p>\r\n\r\n<p>Where $N$ is the number of division points, and the momentum space step:</p>\r\n<p>$$p_m = m \\cdot \\dfrac{2 \\pi \\hbar}{N \\Delta x}, \\qquad m \\in \\{ -\\frac{N}{2}, ..., \\frac{N}{2} \\}$$</p>\r\n\r\n<p>The initial state of the wave function is described by a Gaussian curve that represents a free particle in the scattering problem:</p>\r\n<div class=\"equation\">\r\n  <p>$$\\psi(x,0) = \\frac{1}{\\sqrt{\\sigma \\sqrt{2\\pi}}} \\exp\\left(\\frac{-1}{4}\\left(\\frac{x-x_0}{\\sigma}\\right)^2\\right) {\\rm exp}\\left(i k_0 x \\right)$$</p>\r\n</div>\r\n<p>where $k_0$, $x_0$, and $\\sigma$ are given by the initial conditions.</p>\r\n<p>Due to the Fourier transformation, the system has periodic boundary conditions.</p>\r\n\r\n<hr>\r\n\r\n<p>I also included the beginning of the code showing the needed imports, classes, and functions for the 1D scattering example with comments in English but the whole notebook is on my <a href=\"https://github.com/emmermarcell/Quantum_scattering\" target=\"_blank\">GitHub page</a></p>\r\n\r\n<pre><code class=\"language-python\">\r\nimport numpy as np\r\nfrom scipy.integrate import trapz, simps\r\nimport matplotlib as mpl\r\nfrom matplotlib import animation, rc, pyplot as plt\r\nfrom IPython.display import HTML\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\n\r\nclass Sch:\r\n    \"\"\"\r\n    Class for numerically solving the one-dimensional Schr\u00f6dinger equation.\r\n    All units are in atomic units.\r\n    \"\"\"\r\n    def __init__(self, dt=1., N=1000, m=1, xmin=-250., xaxis=500., x0=0., k0=0.4):\r\n        self.hbar = 1.                    # Reduced Planck constant\r\n        \r\n        # Parameters for position and momentum space\r\n        self.dt = dt\r\n        self.t = 0.\r\n        self.N = N\r\n        self.xaxis = xaxis                # Length of x-axis for the simulation\r\n        self.x = np.linspace(xmin, xmin + self.xaxis, self.N) # x-axis\r\n        self.dx = self.x[1]-self.x[0]\r\n        self.dk = 2 * np.pi * self.hbar / (self.N * self.dx)\r\n        self.k = np.concatenate((np.arange(0, self.N / 2),\r\n                                 np.arange(-self.N / 2, 0))) * self.dk\r\n        \r\n        # Initial value of wave function\r\n        self.m = m\r\n        self.sigma = xaxis/50.            # Spread of the wave packet\r\n        self.x0  = x0/2                   # Expected position of the wave packet\r\n        self.k0 = k0                      # Expected wave number of the wave packet\r\n        self.psi = 1/np.sqrt(self.sigma*np.sqrt(2*np.pi)) * np.exp(-0.25*(self.x-self.x0)**2/self.sigma**2) * np.exp(1j*self.k0*(self.x-self.x0)) \r\n        self.amp = trapz(np.real(np.conj(self.psi)*self.psi), self.x)\r\n        self.psi /= np.sqrt(trapz(np.real(np.conj(self.psi)*self.psi), self.x)) # Normalize the wave packet to 1\r\n        self.psi_k = np.fft.fft(self.psi) # Initial value of wave packet in momentum space\r\n        self.psi_k /= np.sqrt(trapz(np.real(np.conj(self.psi_k)*self.psi_k), self.k)), # Normalize the wave packet to 1\r\n        \r\n        # Hamiltonian operators\r\n        self.Khat = (self.hbar*self.k)**2/(2*m) # Using de-Broglie's equation: p = hbar * k\r\n        self.Vhat = np.zeros(len(self.x))\r\n        \r\n    # Time evolution operators\r\n    def Uk(self):\r\n        return np.exp(-1j*self.dt*self.Khat/self.hbar)\r\n    \r\n    def Uv(self):\r\n        return np.exp(-1j*self.dt*self.Vhat/(2*self.hbar))\r\n    \r\n    def Split(self):\r\n        self.psi *= self.Uv()                 # Apply half of the potential operator\r\n        self.psi = np.fft.fft(self.psi)       # Move to momentum space\r\n        self.psi *= self.Uk()                 # Apply the kinetic energy operator\r\n        self.psi = np.fft.ifft(self.psi)      # Move back to position space\r\n        self.psi *= self.Uv()                 # Apply the other half of the potential operator\r\n        \r\n        self.psi_k = np.fft.fft(self.psi)     # Update the representation in momentum space\r\n        self.psi_k /= np.sqrt(trapz(np.real(np.conj(self.psi_k)*self.psi_k), self.k)), # Normalize the wave packet to 1\r\n        self.t += self.dt                     # Increment time\r\n\r\n        @property\r\n    def Pot(self):\r\n        \"\"\"\r\n        Returns the potential energy function.\r\n        \"\"\"\r\n        return self.Vhat\r\n\r\n    @Pot.setter\r\n    def Pot(self, newV):\r\n        \"\"\"\r\n        Sets a new potential energy function.\r\n        \"\"\"\r\n        if len(newV) == len(self.x):\r\n            self.Vhat = newV\r\n        else:\r\n            raise AttributeError('Invalid potential energy function!')\r\n\r\nwave = Sch()\r\n\r\n# Square potential\r\nV = np.zeros(1000)\r\nV[600:700] = 0.07\r\nwave.Pot = V\r\n\r\n# Plot parameters\r\nmpl.rc('text', usetex = True)\r\nmpl.rc('font', family = 'serif')\r\n\r\nfig, ax = plt.subplots(2, 1)\r\nplt.close()\r\nfig.tight_layout(pad = 3, h_pad=1.5)\r\n\r\nline, = ax[0].plot([], [], lw=1.5, label='$|\\psi|^2$')\r\nlineV, = ax[0].plot([], [], lw=1.5, color='crimson', label='$V$')\r\nlineK, = ax[1].plot([], [], lw=1.5, color='g', label='$|\\psi_k|^2$')\r\n\r\nax[0].set_xlim((-250, 250))\r\nax[0].set_ylim((-0.1, 0.1))\r\nax[0].set_xlabel('$x$ [$a_0$]')\r\nax[0].set_ylabel('$|\\psi|^2$, $V$ [$E_h$]')\r\nax[0].legend(loc='lower right')\r\n\r\nax[1].set_xlim((-2, 2))\r\nax[1].set_ylim((-10, 10))\r\nax[1].set_xlabel('$p$ [$\\hbar / a_0$]')\r\nax[1].set_ylabel('$|\\psi_k|^2$')\r\nax[1].legend(loc='lower right')\r\n\r\n# Initialization function, sets the background for each frame\r\ndef init():\r\n    ax[0].set_title('')\r\n    line.set_data([], [])\r\n    lineV.set_data([], [])\r\n    lineK.set_data([], [])\r\n    return (line, lineV, lineK,)\r\n\r\n# Function responsible for the animation, includes the time evolution\r\ndef animate(i):\r\n    ax[0].set_title('t = '+str(round(wave.t, 1))+' [$\\hbar / E_h$]')\r\n    line.set_data(wave.x, np.real(np.conj(wave.psi)*wave.psi))\r\n    lineV.set_data(wave.x, wave.Vhat)\r\n    lineK.set_data(wave.k, np.real(np.conj(wave.psi_k)*wave.psi_k))\r\n    wave.Split()\r\n    return(line, lineV, lineK,)\r\n\r\nanim = animation.FuncAnimation(fig, animate, init_func=init,\r\n                             frames=500, interval=10, blit=True)\r\nrc('animation', html='jshtml')\r\nanim\r\n</code></pre>\r\n\r\n<p>The code produces the following animated plot:</p>\r\n<p>\r\n<center>\r\n<video width=\"400\" controls>\r\n  <source src='../static/pictures/Scatter_square.mp4' type=\"video/mp4\">\r\n</video>\r\n</center>\r\n</p>\r\n\r\n<p>But just for fun, I show some others where the attractive potential gives the resonance criterion for the incoming wavepacket, and some 2D scattering videos as well for a square wall and for the double slit experiment.</p>\r\n<p>\r\n<center><video width=\"400\" controls>\r\n  <source src='../static/pictures/Scatter_resonance.mp4' type=\"video/mp4\">\r\n</video>\r\n</center></p>\r\n<center>\r\n<video width=\"400\" controls>\r\n  <source src='../static/pictures/Scatter2D_gate.mp4' type=\"video/mp4\">\r\n</video>\r\n<video width=\"400\" controls>\r\n  <source src='../static/pictures/Scatter2D_double_slit.mp4' type=\"video/mp4\">\r\n</video>\r\n</center>\r\n", "paper": ""}, {"id": "3", "date_posted": "2023-11-15", "title": "Quantum Geometric Tensor in the manifold of quantum states", "tags": "Physics, Group Theory, Differential Geometry", "content": "A paper I made for the Hungarian Student Research Societies (TDK) under my professor Dr. P\u00e9ter L\u00e9vay. We were investigating a complex-valued quantity called the quantum geometric tensor (GQT) that exists for parametrized physical systems. Such parameters could be the outer magnetic field in a spin system or a hopping amplitude. Its real part accounts for a metric in the parameter space called the Provost-Valle metric. It is related to the Fubini-study metric living on the projective Hilbert space by a pullback operation. On the other hand, the imaginary part of the QGT is the famous Berry-curvature. We explicitly calculated this quantity for the generalized coherent states (CSs) of the SU(2) and SU(1,1) groups. Furthermore, we introduced the idea of a generalized QGT that is related to the canonical operators instead of the parameter space which can be connected to entanglement. The paper can be found on the official TDK website <a href=\"https://tdk.bme.hu/TTK/MatMatfiz/Kvantumgeometriai-tenzor-a-kvantumallapotok\" target=\"_blank\">here</a>.", "paper": "../static/papers/qgt-tdk.pdf"}, {"id": "4", "date_posted": "2023-11-17", "title": "Weather Forecasting with LSTM models", "tags": "Machine Learning, Programming", "content": "<p>\r\nWeather prediction is a complex problem in physics, falling in the regime of chaotic systems. Recently, deep learning methods have become a popular tool for dealing with them. Google DeepMinds <a href=\"https://arxiv.org/abs/2306.06079\" target=\"_blank\">MetNet-3</a> is a famous and recent attempt to forecast weather variables such as precipitation, temperature, and wind. It's a complex model suited for a complex task, but in my example, I will deal with a toy model simplification of it. I use the publicly available data from the <a href=\"https://www.metnet.hu/napi-adatok?sub=4&pid=15324&date=2023-11\" target=\"_blank\">L\u00e1gym\u00e1nyos automatic weather station of Budapest</a> that provides the Daily maximum and minimum temperature and the precipitation going back to April 2021. After turning these inputs into time-series data frames, I will try to predict the weather for up to 3 days using two connected <a href=\"https://www.sciencedirect.com/science/article/pii/S2212827121003796\" target=\"_blank\">LSTM layers</a> and a Dense layer.\r\n</p>\r\n\r\n<p>\r\nRecurrent neural networks (RNN) and especially long short-term memory models (LSTM) are exceptionally great network architectures for time series prediction. Due to the recurrent nature of the model, it can capture nonlinear short-term time dependencies in its input data and predict future values accordingly.\r\n</p>\r\n\r\n<p>\r\nThe code of the model is the following\r\n</p>\r\n\r\n<script src=\"https://gist.github.com/emmermarcell/ec7d0d5415381e582859b358e78444b8.js\"></script>", "paper": ""}, {"id": "5", "date_posted": "2024-02-08", "title": "Generating synthetic essays with LLMs", "tags": "Machine Learning, NLP, LLM", "content": "<p>\r\nIn recent years, large language models (LLMs) have become increasingly sophisticated, capable of generating text that is difficult to distinguish from human-written text. To get a deeper understanding of their level of expertise Kaggle created a <a href=\"https://www.kaggle.com/competitions/llm-detect-ai-generated-text\" target=\"_blank\">competition that deals with detecting AI-generated text</a>. Alongside two of my brilliant classmates, Izabell J\u00e1r\u00f3, and Kirst\u00f3f Benedek, we entered the competition and compared several models and tokenization methods reaching surprisingly great results (ROC=0.99). \r\n</p>\r\n<p>\r\nMy work included augmenting the original dataset that Kaggle provided, which consisted of only 3 AI-generated essays and 1375 student-written ones. I have used the <code>Mistral-7B-Instruct-v0.1</code> model to generate 400 essays and append them to the popular <a href=\"https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset/versions/2\" target=\"_blank\">DAIGT V2 Train Dataset</a>. My notebook is the following.\r\n</p>\r\n\r\n<p>\r\n<script src=\"https://gist.github.com/emmermarcell/286f60e0bdfc5d92726f14a902fc158c.js\"></script>\r\n</p>\r\n\r\n<p>The notebook does the following things:</p>\r\n<ol>\r\n<li>Introduce the purpose and capabilities of the <code>Mistral-7B-Instruct-v0.1</code> model for generating synthetic essays, emphasizing its use in educational, research, or creative writing contexts.</li>\r\n<li>Explain the importance of formulating clear and detailed prompts to guide the essay generation process effectively, including examples of well-constructed prompts.</li>\r\n<li>Set up authentication and configuration parameters to securely access the LLM.</li>\r\n<li>Demonstrate how to submit a prompt to the <code>Mistral-7B-Instruct-v0.1</code> model using the <code>generate_essay</code> function.</li>\r\n<li>Offer tips for refining and editing the generated essays, including suggestions for post-processing text for coherence, grammar, and style adjustments.</li>\r\n<li>Include optional steps for further customization or experimentation with the <code>Mistral-7B-Instruct-v0.1</code> model, such as adjusting model parameters to vary the tone, style, or depth of the generated essays.</li>\r\n</ol>\r\n\r\n\r\n<p>\r\nOur team's full solution to the competition can be found in our <a href=\"https://github.com/emmermarcell/Detect-AI-Generated-Text/tree/main\" target=\"_blank\">Github Repository</a>.\r\n</p>\r\n\r\n<p>\r\nFurthermore, here is our paper discussing the complete process.\r\n</p>", "paper": "../static/papers/AI_in_Data_Science_Final_Project_LLM_Docs.pdf"}, {"id": "6", "date_posted": "2024-02-17", "title": "Using LLMs for Answering Science-Based Multiple Choice Questions", "tags": "Machine Learning, NLP, LLM", "content": "<p>\r\nThe OpenBookQA dataset imitates open-book exams, a favorite of many students. Here, one can rely on a given context for any question, provided that the student is knowledgeable enough to know where to look. The <a href=\"https://www.kaggle.com/competitions/kaggle-llm-science-exam\" target=\"_blank\">Kaggle - LLM Science Exam</a> competition takes inspiration from this dataset and motivates people to come up with their solutions to solve an exam using LLMs. The exam itself is stored in the form of a dataset containing 200 sample questions with answers labeled as  A, B, C, D, and E. One example is the following:\r\n</p>\r\n\r\n<table>\r\n    <tr>\r\n        <th>prompt</th>\r\n        <th>A</th>\r\n        <th>B</th>\r\n        <th>C</th>\r\n        <th>D</th>\r\n        <th>E</th>\r\n        <th>answer</th>\r\n    </tr>\r\n    <tr>\r\n        <td>What is the most popular explanation for the shower curtain effect?</td>\r\n        <td>The pressure differential between the inside and outside of the shower</td>\r\n        <td>The decrease in vincreasingrease in pressure</td>\r\n        <td>The movement of air across the outside surface of the shower curtain</td>\r\n        <td>The use of cold water</td>\r\n        <td>Bernoulli's principle</td>\r\n        <td>E</td>\r\n    </tr>\r\n</table>\r\n<p>The submissions are evaluated according to the MAP@3 metric</p>\r\n<p>\r\n$$ MAP@3 = \\frac{1}{U} \\sum_{u=1}^U \\sum_{k=1}^{\\min (n,3)} P(k) \\times \\text{rel}(k) $$ \r\n</p>\r\n<p>\r\nwhere U is the number of questions in the test set, P(k) is the precision at cutoff k, n\r\n is the number of predictions per question, and rel(k) is an indicator function equaling 1 if the item at rank k is a relevant (correct) label, and zero otherwise.\r\n</p>\r\n\r\n<p>\r\nThe problem of training LLMs on the dataset provided by Kaggle is the models' lack of \"common and scientific\" knowledge. Throughout the competition, a very popular idea arose, which was to turn the dataset into an \"open-book\" exam, i.e. to provide some sort of a context for each question. My choice of contextual data was the <a href=\"https://www.kaggle.com/datasets/jjinho/wikipedia-20230701\" target=\"_blank\">Wikipedia Plaintext (2023-07-01) dataset on Kaggle</a> that contains Wikipedia articles of all nature up to July 2023. I used the <code>sentence-transformers/all-MiniLM-L6-v2</code> model to embed both the questions and the first sentences of the Wikipedia articles into a 384-dimensional dense vector space where one can perform sentence similarity searches. The <code>sentence-transformers/all-MiniLM-L6-v2</code> model supports the dot-product (inner product), the cosine-similarity, and the simple Euclidean distance as suitable score functions. The Similarity search was done with the <code>IndexFlatIP</code> index of the Faiss library which performs an exact search in the vector space using the inner product (IP) as a score. In my code, I iteratively embedded the context column of the Hugging Face dataset, created from the 1.76 GB <code>wiki_2023_index.parquet</code> file with a batch size of 500'000 into the above-mentioned high-dimensional vector space. Then I added the embedding to the Faiss index. The method of working in batches was used in order not to run out of GPU memory. After that, I also embedded the exam questions located in the <code>prompt</code> column of the wiki_dataset Pandas DataFrame, added them to the index, and performed the similarity search with the <code>retrieve_most_similar</code> function. The similarity search boils down to finding the K-nearest neighbors in the embedding space with the K=1 closest neighbor being the closest article to our question. The context creation for the dataset can be seen in this notebook.\r\n</p>\r\n\r\n<script src=\"https://gist.github.com/emmermarcell/ddd1bfe885d5440378d137aecc651141.js\"></script>\r\n\r\n<p>\r\nThe resulting CSV file is then loaded into a pandas DataFrame and split into training, validation, and test sets with sizes of 100, 50, and 50 entries respectively. I use the <code>microsoft/deberta-v3-large</code> model from Hugging Face with a multiple-choice classification head on top (a linear layer on top of the pooled output and a softmax). I freeze the first 18 out of the 24 layers of the model during the training process and use Weights & Biases Sweeps to perform hyperparameter optimization with the Bayesian search method. The training can be seen in the following notebook.\r\n</p> \r\n\r\n<script src=\"https://gist.github.com/emmermarcell/6ded0bb522640576907dd0add182e578.js\"></script>\r\n\r\n<p>\r\nThe best model was generated in the sweep named <code>glad-sweep-15</code> with a Test MAP@3 value of 0.800. The training arguments and the Weights & Biases summary for this sweep are the following:\r\n</p>\r\n\r\n<div class=\"table-container\">\r\n    <table>\r\n    <caption>Training arguments during glad-sweep-15</caption>\r\n    <tr>\r\n        <th>Argument</th>\r\n        <th>Value</th>\r\n    </tr>\r\n    <tr>\r\n        <td>num_train_epochs</td>\r\n        <td>50</td>\r\n    </tr>\r\n    <tr>\r\n        <td>learning_rate</td>\r\n        <td>3.868708800630949e-05</td>\r\n    </tr>\r\n    <tr>\r\n        <td>weight_decay</td>\r\n        <td>0.03</td>\r\n    </tr>\r\n    <tr>\r\n        <td>warmup_ratio</td>\r\n        <td>0</td>\r\n    </tr>\r\n    <tr>\r\n        <td>gradient_accumulation_steps</td>\r\n        <td>4</td>\r\n    </tr>\r\n    <tr>\r\n        <td>per_device_train_batch_size</td>\r\n        <td>1</td>\r\n    </tr>\r\n    <tr>\r\n        <td>per_device_eval_batch_size</td>\r\n        <td>2</td>\r\n    </tr>\r\n    <tr>\r\n        <td>overwrite_output_dir</td>\r\n        <td>True</td>\r\n    </tr>\r\n    <tr>\r\n        <td>fp16</td>\r\n        <td>True</td>\r\n    </tr>\r\n    <tr>\r\n        <td>logging_steps</td>\r\n        <td>25</td>\r\n    </tr>\r\n    <tr>\r\n        <td>evaluation_strategy</td>\r\n        <td>'steps'</td>\r\n    </tr>\r\n    <tr>\r\n        <td>eval_steps</td>\r\n        <td>25</td>\r\n    </tr>\r\n    <tr>\r\n        <td>save_strategy</td>\r\n        <td>'steps'</td>\r\n    </tr>\r\n    <tr>\r\n        <td>save_steps</td>\r\n        <td>25</td>\r\n    </tr>\r\n    <tr>\r\n        <td>load_best_model_at_end</td>\r\n        <td>True</td>\r\n    </tr>\r\n    <tr>\r\n        <td>metric_for_best_model</td>\r\n        <td>'map@3'</td>\r\n    </tr>\r\n    <tr>\r\n        <td>lr_scheduler_type</td>\r\n        <td>'cosine'</td>\r\n    </tr>\r\n    <tr>\r\n        <td>save_total_limit</td>\r\n        <td>2</td>\r\n    </tr>\r\n</table>\r\n\r\n    <table>\r\n        <caption>Weights & Biases summary of glad-sweep-15</caption>\r\n        <tr>\r\n            <th>Metric</th>\r\n            <th>Value</th>\r\n        </tr>\r\n        <tr><td>train/loss</td><td>0.0426</td></tr>\r\n        <tr><td>train/learning_rate</td><td>0.00002902</td></tr>\r\n        <tr><td>train/epoch</td><td>16</td></tr>\r\n        <tr><td>train/global_step</td><td>200</td></tr>\r\n        <tr><td>_timestamp</td><td>1702060439</td></tr> <!-- Consider how to round this -->\r\n        <tr><td>_runtime</td><td>695.2914</td></tr>\r\n        <tr><td>_step</td><td>17</td></tr>\r\n        <tr><td>eval/loss</td><td>1.9974</td></tr>\r\n        <tr><td>eval/map@3</td><td>0.7567</td></tr>\r\n        <tr><td>eval/accuracy</td><td>0.6</td></tr>\r\n        <tr><td>eval/runtime</td><td>4.6513</td></tr>\r\n        <tr><td>eval/samples_per_second</td><td>10.75</td></tr>\r\n        <tr><td>eval/steps_per_second</td><td>2.795</td></tr>\r\n        <tr><td>train/train_runtime</td><td>594.9302</td></tr>\r\n        <tr><td>train/train_samples_per_second</td><td>8.404</td></tr>\r\n        <tr><td>train/train_steps_per_second</td><td>1.009</td></tr>\r\n        <tr><td>train/total_flos</td><td>977821359296220</td></tr> <!-- Consider whether and how to round -->\r\n        <tr><td>train/train_loss</td><td>0.6246</td></tr>\r\n        <tr><td>Test MAP@3</td><td>0.8</td></tr>\r\n        <tr><td>Test Accuracy</td><td>0.64</td></tr>\r\n        <tr><td>_wandb/runtime</td><td>694</td></tr>\r\n    </table>\r\n</div>\r\n\r\nTo get a deeper understanding of the approach we took to tackle this problem, check out my team's <a href=\"https://github.com/csabi0312/DeepLProject\" target=\"_blank\">Github Repository</a> or the documentation of the code.\r\n\r\n<style>\r\n    /* Custom Table Styles */\r\n    table {\r\n        width: 100%;\r\n        border-collapse: collapse;\r\n        background-color: #292d3e; /* Dark background from your CSS */\r\n        color: #dcdfe4; /* Light text color from your CSS */\r\n        font-family: 'Helvetica Neue', sans-serif; /* Font family from your CSS */\r\n        margin-bottom: 20px; /* Spacing */\r\n    }\r\n    th, td {\r\n        border: 1px solid #44475a; /* Border color from your CSS */\r\n        padding: 10px;\r\n        text-align: left;\r\n    }\r\n    th {\r\n        background-color: #292d3e; /* Matching row background */\r\n        color: #dcdfe4; /* Maintaining text color */\r\n        font-weight: bold; /* Making header text bold */\r\n    }\r\n    tr:nth-child(even) {\r\n        background-color: #1a1a1a; /* Alternating row color for better readability */\r\n    }\r\n    tr:nth-child(odd) {\r\n        background-color: #292d3e; /* Ensuring consistency with even rows */\r\n    }\r\n    caption {\r\n        color: #61afef; /* Lighter and more visible text color */\r\n        font-size: 16px; /* Adjust as needed for readability */\r\n        font-weight: bold; /* Emphasize the caption */\r\n        padding: 10px; /* Space around the text */\r\n        text-align: center; /* Center the caption */\r\n        background-color: #44475a; /* Optional: distinct background color for caption */\r\n        caption-side: top; /* Explicitly place the caption at the top of the table */\r\n    }\r\n</style>\r\n\r\n", "paper": "../static/papers/scientific-question-answering-using-machine-learning-techniques.pdf"}]